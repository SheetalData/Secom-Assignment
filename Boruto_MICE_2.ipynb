{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d496f7fa-de6a-4b9a-96c6-21e6e3f61c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bd994c-fd2f-42f6-ac0c-2cf3911c657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import missingno as msno\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import shapiro\n",
    "# imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url1 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "names = [\"feature\" + str(x) for x in range(1, 591)]\n",
    "df1 = pd.read_csv(url1,sep=\" \", names=names, na_values = \"NaN\",header=None)\n",
    "df1.head()\n",
    "\n",
    "url2 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "df2 = pd.read_csv(url2,sep=\" \",names = [\"Result\",\"Date\"])\n",
    "\n",
    "#df2.columns =['Pass/Fail','Date']\n",
    "df2.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Convertion of Date into Datetime from Object(String) data types\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.dtypes\n",
    "\n",
    "\n",
    "\n",
    "#Joinig TWO df1 and df2 Dataframe naming SECOM\n",
    "Secom = pd.concat([df1,df2],axis = 1)\n",
    "print(Secom)\n",
    "\n",
    "Secom = Secom.drop(['Date'],axis=1)\n",
    "                   \n",
    "# establish target and features of the manufacturing data\n",
    "# set the target to the encoded manufacturing outcome column\n",
    "y = Secom[['Result']]\n",
    "# set the features as the rest of the dataset after dropping the features that are no\n",
    "x = Secom.drop(['Result'], axis=1)\n",
    "\n",
    "# getting the shapes of new data sets x and y\n",
    "print(\"shape of x:\", x.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "#Splitting data\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1,stratify = y)\n",
    "\n",
    "\n",
    "\n",
    "# getting the counts\n",
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train: \", y_train.shape)\n",
    "print(\"shape of y_test: \", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Removing features having Missing ratio more than 50%\n",
    "\n",
    "\n",
    "def percentna(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isnull().sum()/len(dataframe))>threshold]\n",
    "    return columns.tolist()\n",
    "\n",
    " \n",
    "\n",
    "na_columns = percentna(x_train, 0.5)\n",
    "len(na_columns)\n",
    "x_train_dn = x_train.drop(na_columns, axis=1)\n",
    "x_train_dn.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Low Variance Filter\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thres=VarianceThreshold(threshold=0)\n",
    "var_thres.fit(x_train_dn)\n",
    "\n",
    " \n",
    "\n",
    "constant_columns = [column for column in x_train_dn.columns\n",
    "                    if column not in x_train_dn.columns[var_thres.get_support()]]\n",
    "\n",
    "\n",
    "print(len(constant_columns))\n",
    "\n",
    "x_train_lv = x_train_dn.drop(constant_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac6efe-16bc-424e-8165-342824834fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR_outliers(data,limit=1.5):\n",
    "    numColumns = data.select_dtypes(include=np.number).columns.tolist(); # extract list of numeric columns\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3-Q1;\n",
    "    outliers=((data[numColumns] < (Q1 - limit*IQR)) | (data[numColumns] > (Q3 + limit*IQR))).sum()*100/data.shape[0]\n",
    "    return outliers \n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "x_train_lv = x_train_lv.copy()\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state= 42)\n",
    "df_outliers = pd.DataFrame(quantile_transformer.fit_transform(x_train_lv),columns=x_train_lv.columns)\n",
    "outliers = IQR_outliers(df_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76399114-7a1b-44e9-bc5a-ee7c7e4d82f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2437e8-20f1-4b5b-bcb6-1d9f37295a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from impyute.imputation.cs import mice\n",
    "\n",
    "# start the MICE training\n",
    "imputed_training=mice(df_outliers.values)\n",
    "imputed_test=mice(x_test.values)\n",
    "\n",
    "array_sum_train = np.sum(imputed_training)#https://www.adamsmith.haus/python/answers/how-to-check-for-nan-elements-in-a-numpy-array-in-python\n",
    "array_sum_test = np.sum(imputed_test) \n",
    "Trainset = np.isnan(array_sum_train)\n",
    "Testset = np.isnan(array_sum_test)\n",
    "\n",
    "Trainset\n",
    "Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a5395c-7e64-4138-8d61-0dcc242150dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f7b44-1ff1-4f7e-93a6-1ccc8d616993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BORUTA\n",
    "#REFERENCE: https://github.com/bnsreenu/python_for_microscopists/blob/master/198_Boruta_feature_selection_breast_cancer.py\n",
    "#pip install boruta\n",
    " \n",
    "#Standarize train data for BORUTA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_std = sc.fit_transform(Trainset)\n",
    "x_test_std = sc.transform(Testset)\n",
    " \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    " \n",
    "# load y_train as an array\n",
    " \n",
    "y = y_train.values\n",
    "y = y.ravel()\n",
    " \n",
    "# define random forest classifier, with utilising all cores and\n",
    "# sampling in proportion to y labels\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    " \n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    " \n",
    "# find all relevant features \n",
    "feat_selector.fit(x_train_std, y)\n",
    " \n",
    "# check selected features \n",
    "feat_selector.support_\n",
    " \n",
    "# check ranking of features\n",
    "feat_selector.ranking_\n",
    " \n",
    "# call transform() on X to filter it down to selected features\n",
    "X_filtered = feat_selector.transform(x_train_std)\n",
    " \n",
    "import numpy as np\n",
    "feature_names = np.array(Secom.columns)\n",
    " \n",
    "# Ranked features greater than threshold\n",
    "feature_ranks = list(zip(feature_names, \n",
    " feat_selector.ranking_, \n",
    " feat_selector.support_))\n",
    " \n",
    "# print the results\n",
    "for feat in feature_ranks:\n",
    " print('Feature: {:<30} Rank: {}, Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    " \n",
    "#Now use the subset of features to fit XGBoost model on training data\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier()\n",
    " \n",
    "xgb_model.fit(X_filtered, y_train)\n",
    " \n",
    "#Now predict on test data using the trained model. \n",
    " \n",
    "#First apply feature selector transform to make sure same features are selected from test data\n",
    "X_test_filtered = feat_selector.transform(x_test_std)\n",
    "prediction_xgb = xgb_model.predict(X_test_filtered)\n",
    " \n",
    "#Print overall accuracy\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_xgb))\n",
    " \n",
    "#Confusion Matrix - verify accuracy of each class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, prediction_xgb)\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
