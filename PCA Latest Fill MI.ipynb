{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a265ed-1288-40a3-8e99-986399510b18",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Missing value imputation using Pandas latest filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742549eb-0ee2-4fa3-860b-4deac04ff79b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de20660e-f179-44b6-84e4-4b4260670384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import missingno as msno\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import shapiro\n",
    "# imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url1 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "names = [\"feature\" + str(x) for x in range(1, 591)]\n",
    "df1 = pd.read_csv(url1,sep=\" \", names=names, na_values = \"NaN\",header=None)\n",
    "df1.head()\n",
    "\n",
    "url2 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "df2 = pd.read_csv(url2,sep=\" \",names = [\"Result\",\"Date\"])\n",
    "\n",
    "#df2.columns =['Pass/Fail','Date']\n",
    "df2.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Convertion of Date into Datetime from Object(String) data types\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.dtypes\n",
    "\n",
    "\n",
    "\n",
    "#Joinig TWO df1 and df2 Dataframe naming SECOM\n",
    "Secom = pd.concat([df1,df2],axis = 1)\n",
    "print(Secom)\n",
    "\n",
    "Secom = Secom.drop(['Date'],axis=1)\n",
    "                   \n",
    "# establish target and features of the manufacturing data\n",
    "# set the target to the encoded manufacturing outcome column\n",
    "y = Secom[['Result']]\n",
    "# set the features as the rest of the dataset after dropping the features that are no\n",
    "x = Secom.drop(['Result'], axis=1)\n",
    "\n",
    "# getting the shapes of new data sets x and y\n",
    "print(\"shape of x:\", x.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "#Splitting data\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1,stratify = y)\n",
    "\n",
    "\n",
    "\n",
    "# getting the counts\n",
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train: \", y_train.shape)\n",
    "print(\"shape of y_test: \", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Removing features having Missing ratio more than 50%\n",
    "\n",
    "\n",
    "def percentna(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isnull().sum()/len(dataframe))>threshold]\n",
    "    return columns.tolist()\n",
    "\n",
    " \n",
    "\n",
    "na_columns = percentna(x_train, 0.5)\n",
    "len(na_columns)\n",
    "x_train_dn = x_train.drop(na_columns, axis=1)\n",
    "x_train_dn.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Low Variance Filter\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thres=VarianceThreshold(threshold=0)\n",
    "var_thres.fit(x_train_dn)\n",
    "\n",
    " \n",
    "\n",
    "constant_columns = [column for column in x_train_dn.columns\n",
    "                    if column not in x_train_dn.columns[var_thres.get_support()]]\n",
    "\n",
    "\n",
    "print(len(constant_columns))\n",
    "\n",
    "x_train_lv = x_train_dn.drop(constant_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0169d40-f7a0-4798-af51-823f3f1cca7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9550b9-6202-4d2f-bb2b-3dfc0e4e0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(feat):\n",
    " upper_limit = feat.mean() + 3*feat.std()\n",
    " lower_limit = feat.mean() - 3*feat.std()\n",
    "\n",
    " feat = np.where(\n",
    "    feat >upper_limit,\n",
    "    upper_limit,\n",
    "    np.where(\n",
    "       feat <lower_limit,\n",
    "        lower_limit,\n",
    "        feat ))\n",
    " return feat\n",
    "\n",
    "x_train_outliers_imputation =x_train_lv.copy()\n",
    "for column in x_train_outliers_imputation:\n",
    "  x_train_outliers_imputation[column] = outliers(x_train_outliers_imputation[column])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844ce410-4259-4cd4-9a3d-512181f9af88",
   "metadata": {},
   "source": [
    "Method to use for filling holes in reindexed Series pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d065a530-87ab-49a0-91ac-fa913f40602b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411db0b-cec6-4165-b396-d5bb00109139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# latest information available\n",
    "x_train_LastFill1 = x_train_outliers_imputation.copy()\n",
    "x_train_LastFill1.fillna(method='ffill', inplace=True)\n",
    "x_train_LastFill1.fillna(method='bfill', inplace=True)\n",
    "x_train_LastFill1\n",
    "\n",
    "x_train_LastFill1.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ba494d-d061-4fba-9a02-b2444706a198",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_LastFill1 = x_test.copy()\n",
    "x_test_LastFill1.fillna(method='ffill', inplace=True)\n",
    "x_test_LastFill1.fillna(method='bfill', inplace=True)\n",
    "x_test_LastFill1\n",
    "\n",
    "x_test_LastFill1.isna().sum().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506fe732-30d9-44c7-a9fc-9b9428c3241f",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1178c109-048b-4213-aecb-940fbbfd05ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_Hot_deck2 = x_train_Hot_deck2.replace(np.NaN, 0)\n",
    "#x_train_Hot_deck2.isnull().any().any()\n",
    "#x_test_Hot_deck2 = x_test_Hot_deck2.replace(np.NaN, 0)\n",
    "#x_test_Hot_deck2.isnull().any().any()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "col=list(x_train_LastFill1.columns)\n",
    "col1=list(x_test_LastFill1.columns)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_feature = scaler.fit_transform(x_train_LastFill1)\n",
    "scaled_feature = pd.DataFrame(scaled_feature,columns=col)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "# Creating a covariance matrix\n",
    "\n",
    "cov_matrix = np.cov(scaled_feature.T)\n",
    "print('Covariance Matrix \\n', cov_matrix)\n",
    "      \n",
    "#perform an eigendecomposition on the covariance matrix:\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)  \n",
    "\n",
    "\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cumulative Variance Explained\", cum_var_exp)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12 ,6))\n",
    "plt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "pca = PCA(n_components=scaled_feature.shape[1],random_state=1).fit(scaled_feature.values)\n",
    "a = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "i = 0\n",
    "while a[i] < 0.9:\n",
    "    i = i+1\n",
    "f'Number of dimensions needed to capture 90%% of variance:{i}'\n",
    "\n",
    "\n",
    "# PCA with reduced number of components\n",
    "\n",
    "pca = PCA(n_components=i,random_state=1)\n",
    "pca.fit(scaled_feature)\n",
    "#print(pca.components_)\n",
    "\n",
    "pca_df= pd.DataFrame(pca.fit_transform(scaled_feature))\n",
    "print(pca_df)\n",
    "\n",
    "\n",
    "pca_df.shape\n",
    "\n",
    "#heatmap before and after PCA\n",
    "# after PCA shows that the components are in the acceptable range of colinearity: means that the components are independent and  suitable for model making.\n",
    "f, (ax_heatmap, ax_heatmap2) = plt.subplots(2,figsize=(16,12))\n",
    "ax_heatmap.set_title('Before PCA')\n",
    "ax_heatmap2.set_title('After PCA')\n",
    "\n",
    "sns.heatmap(scaled_feature,ax=ax_heatmap,cmap='plasma');\n",
    "sns.heatmap(pca_df,ax=ax_heatmap2,cmap='plasma');\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# create a PCA object\n",
    "pca = PCA(n_components = i)# extracted features we want to end up within our new dataset(2).\n",
    "# Apply the above object to our training dataset using the fit method.\n",
    "X_train = pca.fit_transform(x_train_lastFill1)\n",
    "# Apply the PCA object to the test set only to transform this set\n",
    "X_test = pca.fit_transform(x_test_LastFill1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fba4afb-a05b-4032-a09a-f1d70e254205",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c07239-a148-4905-9f8c-60792aceaa8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "classifiers = [['Naive Bayes :', GaussianNB()],\n",
    "               ['KNeighbours :', KNeighborsClassifier()],\n",
    "               ['SVM :', SVC()],\n",
    "               ['LogisticRegression :', LogisticRegression()],\n",
    "               ['DecisionTree :',DecisionTreeClassifier()],\n",
    "               ['RandomForest :',RandomForestClassifier()]]\n",
    "\n",
    "for name,classifier in classifiers:\n",
    "    clf=classifier.fit(X_train,y_train)\n",
    "    y_pred=classifier.predict(X_test)\n",
    "    print(f'\\n {name} \\n')\n",
    "    print(f'Training Score for {name}  {clf.score(X_train,y_train) * 100:.2f}' )\n",
    "    print(f'Testing Score for {name} {clf.score(X_test,y_test) * 100:.2f}' )\n",
    "    print(f'Classification report  \\n {classification_report(y_test,y_pred)}' )\n",
    "    print(f'Confusion matrix  \\n {confusion_matrix(y_test,y_pred)}' )\n",
    "    print(f'ROC AUC  : {roc_auc_score(y_test,y_pred)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91335371-991d-4168-8080-b5c3f8cdd687",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
