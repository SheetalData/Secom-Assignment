{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b10f8c0-8bbc-4d37-826f-9d839232b2c5",
   "metadata": {},
   "source": [
    "### Missing value imputation using KNN Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44a514c-4eff-4340-a749-9effb1f6ecaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "84e2e26c-cf06-4ca5-83ee-d154a87dcc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature1  feature2   feature3   feature4  feature5  feature6  feature7  \\\n",
      "0      3030.93   2564.00  2187.7333  1411.1265    1.3602     100.0   97.6133   \n",
      "1      3095.78   2465.14  2230.4222  1463.6606    0.8294     100.0  102.3433   \n",
      "2      2932.61   2559.94  2186.4111  1698.0172    1.5102     100.0   95.4878   \n",
      "3      2988.72   2479.90  2199.0333   909.7926    1.3204     100.0  104.2367   \n",
      "4      3032.24   2502.87  2233.3667  1326.5200    1.5334     100.0  100.3967   \n",
      "...        ...       ...        ...        ...       ...       ...       ...   \n",
      "1562   2899.41   2464.36  2179.7333  3085.3781    1.4843     100.0   82.2467   \n",
      "1563   3052.31   2522.55  2198.5667  1124.6595    0.8763     100.0   98.4689   \n",
      "1564   2978.81   2379.78  2206.3000  1110.4967    0.8236     100.0   99.4122   \n",
      "1565   2894.92   2532.01  2177.0333  1183.7287    1.5726     100.0   98.7978   \n",
      "1566   2944.92   2450.76  2195.4444  2914.1792    1.5978     100.0   85.1011   \n",
      "\n",
      "      feature8  feature9  feature10  ...  feature583  feature584  feature585  \\\n",
      "0       0.1242    1.5005     0.0162  ...      0.5005      0.0118      0.0035   \n",
      "1       0.1247    1.4966    -0.0005  ...      0.5019      0.0223      0.0055   \n",
      "2       0.1241    1.4436     0.0041  ...      0.4958      0.0157      0.0039   \n",
      "3       0.1217    1.4882    -0.0124  ...      0.4990      0.0103      0.0025   \n",
      "4       0.1235    1.5031    -0.0031  ...      0.4800      0.4766      0.1045   \n",
      "...        ...       ...        ...  ...         ...         ...         ...   \n",
      "1562    0.1248    1.3424    -0.0045  ...      0.4988      0.0143      0.0039   \n",
      "1563    0.1205    1.4333    -0.0061  ...      0.4975      0.0131      0.0036   \n",
      "1564    0.1208       NaN        NaN  ...      0.4987      0.0153      0.0041   \n",
      "1565    0.1213    1.4622    -0.0072  ...      0.5004      0.0178      0.0038   \n",
      "1566    0.1235       NaN        NaN  ...      0.4987      0.0181      0.0040   \n",
      "\n",
      "      feature586  feature587  feature588  feature589  feature590  Result  \\\n",
      "0         2.3630         NaN         NaN         NaN         NaN      -1   \n",
      "1         4.4447      0.0096      0.0201      0.0060    208.2045      -1   \n",
      "2         3.1745      0.0584      0.0484      0.0148     82.8602       1   \n",
      "3         2.0544      0.0202      0.0149      0.0044     73.8432      -1   \n",
      "4        99.3032      0.0202      0.0149      0.0044     73.8432      -1   \n",
      "...          ...         ...         ...         ...         ...     ...   \n",
      "1562      2.8669      0.0068      0.0138      0.0047    203.1720      -1   \n",
      "1563      2.6238      0.0068      0.0138      0.0047    203.1720      -1   \n",
      "1564      3.0590      0.0197      0.0086      0.0025     43.5231      -1   \n",
      "1565      3.5662      0.0262      0.0245      0.0075     93.4941      -1   \n",
      "1566      3.6275      0.0117      0.0162      0.0045    137.7844      -1   \n",
      "\n",
      "                    Date  \n",
      "0    2008-07-19 11:55:00  \n",
      "1    2008-07-19 12:32:00  \n",
      "2    2008-07-19 13:17:00  \n",
      "3    2008-07-19 14:43:00  \n",
      "4    2008-07-19 15:22:00  \n",
      "...                  ...  \n",
      "1562 2008-10-16 15:13:00  \n",
      "1563 2008-10-16 20:49:00  \n",
      "1564 2008-10-17 05:26:00  \n",
      "1565 2008-10-17 06:01:00  \n",
      "1566 2008-10-17 06:07:00  \n",
      "\n",
      "[1567 rows x 592 columns]\n",
      "shape of x: (1567, 590)\n",
      "shape of y: (1567, 1)\n",
      "shape of x_train:  (1096, 590)\n",
      "shape of x_test:  (471, 590)\n",
      "shape of y_train:  (1096, 1)\n",
      "shape of y_test:  (471, 1)\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import missingno as msno\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import shapiro\n",
    "# imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url1 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "names = [\"feature\" + str(x) for x in range(1, 591)]\n",
    "df1 = pd.read_csv(url1,sep=\" \", names=names, na_values = \"NaN\",header=None)\n",
    "df1.head()\n",
    "\n",
    "url2 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "df2 = pd.read_csv(url2,sep=\" \",names = [\"Result\",\"Date\"])\n",
    "\n",
    "#df2.columns =['Pass/Fail','Date']\n",
    "df2.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Convertion of Date into Datetime from Object(String) data types\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.dtypes\n",
    "\n",
    "\n",
    "\n",
    "#Joinig TWO df1 and df2 Dataframe naming SECOM\n",
    "Secom = pd.concat([df1,df2],axis = 1)\n",
    "print(Secom)\n",
    "\n",
    "Secom = Secom.drop(['Date'],axis=1)\n",
    "                   \n",
    "# establish target and features of the manufacturing data\n",
    "# set the target to the encoded manufacturing outcome column\n",
    "y = Secom[['Result']]\n",
    "# set the features as the rest of the dataset after dropping the features that are no\n",
    "x = Secom.drop(['Result'], axis=1)\n",
    "\n",
    "# getting the shapes of new data sets x and y\n",
    "print(\"shape of x:\", x.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "#Splitting data\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1,stratify = y)\n",
    "\n",
    "\n",
    "\n",
    "# getting the counts\n",
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train: \", y_train.shape)\n",
    "print(\"shape of y_test: \", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Removing features having Missing ratio more than 50%\n",
    "\n",
    "\n",
    "def percentna(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isnull().sum()/len(dataframe))>threshold]\n",
    "    return columns.tolist()\n",
    "\n",
    " \n",
    "\n",
    "na_columns = percentna(x_train, 0.5)\n",
    "len(na_columns)\n",
    "x_train_dn = x_train.drop(na_columns, axis=1)\n",
    "x_train_dn.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Low Variance Filter\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thres=VarianceThreshold(threshold=0)\n",
    "var_thres.fit(x_train_dn)\n",
    "\n",
    " \n",
    "\n",
    "constant_columns = [column for column in x_train_dn.columns\n",
    "                    if column not in x_train_dn.columns[var_thres.get_support()]]\n",
    "\n",
    "\n",
    "print(len(constant_columns))\n",
    "\n",
    "x_train_lv = x_train_dn.drop(constant_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9550b9-6202-4d2f-bb2b-3dfc0e4e0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(feat):\n",
    " upper_limit = feat.mean() + 3*feat.std()\n",
    " lower_limit = feat.mean() - 3*feat.std()\n",
    "\n",
    " feat = np.where(\n",
    "    feat >upper_limit,\n",
    "    upper_limit,\n",
    "    np.where(\n",
    "       feat <lower_limit,\n",
    "        lower_limit,\n",
    "        feat ))\n",
    " return feat\n",
    "\n",
    "x_train_outliers_imputation =x_train_lv.copy()\n",
    "for column in x_train_outliers_imputation:\n",
    "  x_train_outliers_imputation[column] = outliers(x_train_outliers_imputation[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d95be323-b48c-401d-b3ed-5076e1c3746e",
   "metadata": {},
   "outputs": [],
   "source": [
    "numColumns = x_train_outliers_imputation.select_dtypes(include=np.number).columns.tolist();\n",
    "#numColumns2 = x_test.select_dtypes(include=np.number).columns.tolist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db2a7873-325b-40f4-b045-80596e969b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize imputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "\n",
    "\n",
    "# fit the imputer on X_train. pass only numeric columns.\n",
    "imputer.fit(x_train_outliers_imputation[numColumns])\n",
    "\n",
    "\n",
    "\n",
    "# transform the data using the fitted imputer\n",
    "X_train_knn_impute1 = imputer.transform(x_train_outliers_imputation[numColumns])\n",
    "X_test_knn_impute1 = imputer.transform(x_test[numColumns])\n",
    "\n",
    "\n",
    "\n",
    "# put the output into DataFrame. remember to pass columns used in fit/transform\n",
    "X_train_knn_impute1 = pd.DataFrame(X_train_knn_impute1, columns=numColumns)\n",
    "X_test_knn_impute1 = pd.DataFrame(X_test_knn_impute1, columns=numColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d92a30-2d26-4039-9ce4-1df13a39cca7",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1178c109-048b-4213-aecb-940fbbfd05ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance Matrix \n",
      " [[ 0.02653982 -0.00419749 -0.00050406 ... -0.00058656 -0.00072725\n",
      "  -0.00042327]\n",
      " [-0.00419749  0.02585938  0.00135255 ...  0.00060217  0.00022807\n",
      "   0.00107126]\n",
      " [-0.00050406  0.00135255  0.02529192 ... -0.00134907 -0.00118887\n",
      "  -0.00121662]\n",
      " ...\n",
      " [-0.00058656  0.00060217 -0.00134907 ...  0.04180563  0.04087559\n",
      "   0.01890297]\n",
      " [-0.00072725  0.00022807 -0.00118887 ...  0.04087559  0.04238368\n",
      "   0.01892012]\n",
      " [-0.00042327  0.00107126 -0.00121662 ...  0.01890297  0.01892012\n",
      "   0.04448709]]\n",
      "Cumulative Variance Explained [  4.62898836   7.78413099  10.74959777  13.32406358  15.55107643\n",
      "  17.65434876  19.58272142  21.41662848  23.16893582  24.88189773\n",
      "  26.55655017  28.20791653  29.73886726  31.14212347  32.49039156\n",
      "  33.78542803  35.06548266  36.31440196  37.53195873  38.6504444\n",
      "  39.75183842  40.83361923  41.88628786  42.93512851  43.94218354\n",
      "  44.92327995  45.88881688  46.8416025   47.76706215  48.67729888\n",
      "  49.58010116  50.44865831  51.31591395  52.15154212  52.98194169\n",
      "  53.78950865  54.58621497  55.37828763  56.14177792  56.9001868\n",
      "  57.65552809  58.397793    59.1295876   59.85100219  60.55791566\n",
      "  61.25104951  61.93006093  62.59330621  63.24427788  63.88520679\n",
      "  64.51815981  65.14392511  65.75223413  66.35687666  66.94728795\n",
      "  67.53021112  68.10120735  68.65817322  69.20216506  69.74167286\n",
      "  70.27040152  70.78556225  71.29169946  71.78913847  72.2841224\n",
      "  72.76953726  73.24135119  73.70678818  74.16356956  74.61872659\n",
      "  75.06701425  75.49331468  75.91370348  76.33238714  76.73946399\n",
      "  77.13833056  77.53509882  77.93008564  78.32002312  78.69394702\n",
      "  79.06440926  79.43340723  79.79352067  80.15081007  80.50188305\n",
      "  80.84718314  81.18988507  81.52395945  81.85433771  82.17767064\n",
      "  82.49361789  82.8079246   83.11139015  83.41213524  83.70670327\n",
      "  83.99919394  84.28914085  84.57449743  84.85390773  85.12985619\n",
      "  85.40408741  85.67454695  85.94036393  86.20384302  86.46169188\n",
      "  86.7123235   86.96082173  87.20146839  87.44064683  87.67886828\n",
      "  87.91050507  88.14118377  88.36716744  88.59000006  88.81038929\n",
      "  89.02822155  89.24020185  89.45046107  89.65564933  89.85873552\n",
      "  90.0603966   90.25794476  90.45391763  90.64258237  90.8300673\n",
      "  91.01528462  91.19872065  91.37838215  91.55603074  91.73189529\n",
      "  91.90536921  92.07734567  92.24688798  92.41249333  92.57793022\n",
      "  92.73583643  92.89124369  93.0447648   93.19698727  93.34726884\n",
      "  93.49489538  93.64031105  93.78271116  93.92150173  94.05909752\n",
      "  94.19337453  94.32547469  94.45545185  94.58431178  94.71091835\n",
      "  94.83357857  94.95441759  95.07302313  95.18838029  95.30063558\n",
      "  95.4112907   95.52030424  95.62840401  95.73390128  95.8354681\n",
      "  95.9359475   96.03469102  96.1314273   96.22695071  96.32190003\n",
      "  96.41425461  96.50438018  96.59333549  96.67984565  96.76472511\n",
      "  96.84813041  96.93071208  97.01242078  97.093346    97.17075952\n",
      "  97.24569364  97.31871411  97.38956334  97.45709425  97.52419431\n",
      "  97.58984405  97.6533018   97.71616137  97.77672028  97.83599666\n",
      "  97.89442595  97.95277493  98.00864555  98.06276305  98.11448017\n",
      "  98.1643388   98.21326833  98.26102557  98.30509307  98.34874707\n",
      "  98.39199771  98.43480747  98.47678804  98.51811708  98.55785074\n",
      "  98.59552111  98.63189825  98.6677583   98.7026059   98.73595847\n",
      "  98.76836745  98.8007003   98.83212893  98.8621079   98.89120329\n",
      "  98.91958952  98.94705801  98.97394979  99.00034349  99.02600933\n",
      "  99.05130246  99.07582202  99.09924811  99.12139762  99.14275894\n",
      "  99.16388386  99.18425337  99.2037204   99.22274296  99.24100182\n",
      "  99.25904807  99.27697681  99.29434541  99.31134126  99.32794799\n",
      "  99.34402422  99.3598786   99.37524669  99.3902732   99.40445487\n",
      "  99.41834297  99.43185563  99.44504713  99.45804218  99.47084334\n",
      "  99.48338152  99.49562374  99.5075658   99.51906056  99.53043236\n",
      "  99.54140774  99.55214338  99.56262877  99.57288682  99.58295343\n",
      "  99.59288976  99.60265375  99.61225985  99.62169374  99.63090783\n",
      "  99.63986942  99.64850979  99.65702042  99.6654631   99.67389256\n",
      "  99.68211856  99.69004447  99.6978797   99.70554206  99.71305267\n",
      "  99.7204391   99.72776285  99.7349009   99.74181841  99.74850096\n",
      "  99.75514556  99.76176298  99.76815194  99.77438006  99.78055442\n",
      "  99.78662461  99.79263336  99.79861546  99.80442192  99.81000108\n",
      "  99.8154667   99.82081946  99.8258976   99.83096488  99.83599193\n",
      "  99.84092633  99.84567667  99.85036862  99.85500094  99.85956119\n",
      "  99.86399474  99.86837172  99.87262007  99.87683269  99.8809808\n",
      "  99.88502356  99.8890026   99.89287452  99.89658152  99.90027015\n",
      "  99.90377455  99.90717951  99.91055852  99.91389139  99.91710925\n",
      "  99.92019291  99.92324188  99.92613906  99.92898697  99.93179182\n",
      "  99.93452905  99.93708663  99.93950019  99.9418116   99.94410352\n",
      "  99.9463167   99.94842677  99.95045737  99.95243665  99.95437745\n",
      "  99.95625092  99.95808643  99.95981665  99.96148152  99.9631378\n",
      "  99.96472852  99.96626865  99.96775512  99.96921028  99.97055094\n",
      "  99.97185709  99.97310062  99.97431168  99.97545741  99.97655546\n",
      "  99.97760502  99.97860798  99.97959556  99.98051652  99.98142564\n",
      "  99.98227156  99.98309934  99.98390086  99.98468142  99.98543663\n",
      "  99.98618938  99.98689598  99.98756507  99.98821416  99.98884797\n",
      "  99.98946647  99.99005774  99.99062553  99.9911533   99.99165358\n",
      "  99.99213616  99.99258269  99.99301468  99.99341773  99.99380804\n",
      "  99.9941461   99.99447377  99.99477859  99.99507601  99.99534164\n",
      "  99.99560372  99.99584906  99.99607322  99.99628768  99.99648259\n",
      "  99.99666777  99.99684996  99.99702719  99.99719441  99.99735755\n",
      "  99.99751471  99.9976634   99.99780398  99.99793949  99.99807137\n",
      "  99.99819359  99.99830303  99.99840482  99.99850523  99.99860257\n",
      "  99.99869342  99.99878189  99.99886933  99.99894666  99.99902168\n",
      "  99.99909418  99.99916592  99.9992341   99.99929608  99.9993565\n",
      "  99.99941164  99.99946472  99.99950352  99.9995402   99.99957221\n",
      "  99.99960259  99.99963209  99.99965953  99.99968608  99.9997117\n",
      "  99.99973641  99.99976054  99.99978312  99.99980316  99.99982205\n",
      "  99.99983982  99.99985671  99.99987286  99.99988655  99.99989989\n",
      "  99.99991001  99.99991857  99.99992644  99.99993347  99.99994017\n",
      "  99.99994676  99.99995309  99.99995845  99.99996295  99.99996721\n",
      "  99.99997129  99.99997506  99.99997868  99.99998208  99.99998519\n",
      "  99.99998821  99.99999113  99.99999367  99.99999607  99.99999786\n",
      "  99.99999882  99.99999948  99.99999982  99.9999999   99.99999994\n",
      "  99.99999998  99.99999999 100.         100.         100.\n",
      " 100.         100.         100.         100.         100.        ]\n",
      "           0         1         2         3         4         5         6    \\\n",
      "0    -0.456953 -0.111895  0.606789  0.060326 -0.369474 -0.209459 -1.796825   \n",
      "1    -0.902157 -0.402948 -0.153964 -0.469109  1.498979  0.482939 -1.006092   \n",
      "2     1.170385 -1.021500  0.266500  0.215983 -0.403850 -0.250972  0.424438   \n",
      "3     0.670824 -0.285152 -0.146346 -0.337865 -0.299089 -0.530458 -0.765368   \n",
      "4     1.337966 -0.880351  0.420631 -0.373063  0.269348 -0.396717  0.102756   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1091  0.586760 -0.698348  0.679897  0.229813 -0.290801 -0.056995  0.370818   \n",
      "1092 -0.969965  0.153142 -0.722157  0.039742  0.583553  0.331260 -0.022852   \n",
      "1093 -0.328449 -0.246419 -0.626976 -0.728644  0.826001  0.356596 -0.074187   \n",
      "1094 -0.945709  0.211379  0.275298  0.135761  0.134761  0.368004  0.492592   \n",
      "1095 -0.102519  0.240143 -0.362433 -1.192332 -0.155749  0.097320 -1.099695   \n",
      "\n",
      "           7         8         9    ...       110       111       112  \\\n",
      "0    -0.183760 -0.001568  1.264372  ...  0.047300  0.161927 -0.017044   \n",
      "1    -0.662513  0.207950  1.342717  ... -0.281895 -0.306668  0.273607   \n",
      "2    -0.639123  0.021899 -0.043167  ...  0.140318  0.204018  0.021073   \n",
      "3    -0.123416  0.033100 -0.490666  ...  0.212605 -0.189413 -0.047968   \n",
      "4     0.215523  0.320264 -0.399279  ...  0.027104 -0.017816 -0.035821   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1091  0.525531 -0.059551  0.348814  ...  0.036945 -0.182310 -0.280188   \n",
      "1092  0.259891  0.306347 -0.363928  ...  0.050445  0.034956 -0.324754   \n",
      "1093 -0.258532  0.677160 -1.204478  ...  0.012738 -0.008570 -0.295908   \n",
      "1094  0.100878  0.097590  0.584137  ...  0.123093 -0.087847  0.023584   \n",
      "1095 -0.201884 -0.645902  0.152104  ...  0.038543 -0.090585 -0.026355   \n",
      "\n",
      "           113       114       115       116       117       118       119  \n",
      "0     0.224772  0.102908 -0.257784 -0.044429 -0.011086  0.242016  0.146464  \n",
      "1     0.072435 -0.304969 -0.215372  0.347571 -0.142763 -0.198646 -0.041007  \n",
      "2     0.043237  0.158658  0.103836  0.103824  0.181980  0.178629 -0.016393  \n",
      "3    -0.131417 -0.220616 -0.123964 -0.013725  0.059405 -0.071404  0.074288  \n",
      "4    -0.364820 -0.003188 -0.212968 -0.219037 -0.079495 -0.394656  0.083930  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "1091 -0.065592  0.413829 -0.199289  0.113439  0.459133  0.033617 -0.053780  \n",
      "1092 -0.075062 -0.319747 -0.013936 -0.056056 -0.051243  0.028753 -0.164338  \n",
      "1093  0.146333 -0.141678 -0.015749 -0.156692  0.199127  0.213442  0.207109  \n",
      "1094  0.148600 -0.039779  0.041720 -0.079843 -0.001101  0.103313  0.107635  \n",
      "1095  0.244637 -0.035022  0.130867 -0.274814 -0.164989 -0.039180  0.032517  \n",
      "\n",
      "[1096 rows x 120 columns]\n"
     ]
    }
   ],
   "source": [
    "#x_train_Hot_deck2 = x_train_Hot_deck2.replace(np.NaN, 0)\n",
    "#x_train_Hot_deck2.isnull().any().any()\n",
    "#x_test_Hot_deck2 = x_test_Hot_deck2.replace(np.NaN, 0)\n",
    "#x_test_Hot_deck2.isnull().any().any()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "col=list(X_train_knn_impute1.columns)\n",
    "col1=list(X_test_knn_impute1.columns)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_feature = scaler.fit_transform(X_train_knn_impute1)\n",
    "scaled_feature = pd.DataFrame(scaled_feature,columns=col)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "# Creating a covariance matrix\n",
    "\n",
    "cov_matrix = np.cov(scaled_feature.T)\n",
    "print('Covariance Matrix \\n', cov_matrix)\n",
    "      \n",
    "#perform an eigendecomposition on the covariance matrix:\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)  \n",
    "\n",
    "\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cumulative Variance Explained\", cum_var_exp)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12 ,6))\n",
    "plt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "pca = PCA(n_components=scaled_feature.shape[1],random_state=1).fit(scaled_feature.values)\n",
    "a = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "i = 0\n",
    "while a[i] < 0.9:\n",
    "    i = i+1\n",
    "f'Number of dimensions needed to capture 90%% of variance:{i}'\n",
    "\n",
    "\n",
    "# PCA with reduced number of components\n",
    "\n",
    "pca = PCA(n_components=i,random_state=1)\n",
    "pca.fit(scaled_feature)\n",
    "#print(pca.components_)\n",
    "\n",
    "pca_df= pd.DataFrame(pca.fit_transform(scaled_feature))\n",
    "print(pca_df)\n",
    "\n",
    "\n",
    "pca_df.shape\n",
    "\n",
    "#heatmap before and after PCA\n",
    "# after PCA shows that the components are in the acceptable range of colinearity: means that the components are independent and  suitable for model making.\n",
    "f, (ax_heatmap, ax_heatmap2) = plt.subplots(2,figsize=(16,12))\n",
    "ax_heatmap.set_title('Before PCA')\n",
    "ax_heatmap2.set_title('After PCA')\n",
    "\n",
    "sns.heatmap(scaled_feature,ax=ax_heatmap,cmap='plasma');\n",
    "sns.heatmap(pca_df,ax=ax_heatmap2,cmap='plasma');\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# create a PCA object\n",
    "pca = PCA(n_components = i)# extracted features we want to end up within our new dataset(2).\n",
    "# Apply the above object to our training dataset using the fit method.\n",
    "X_train = pca.fit_transform(X_train_knn_impute1)\n",
    "# Apply the PCA object to the test set only to transform this set\n",
    "X_test = pca.fit_transform(X_test_knn_impute1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a27e9-cbab-4eec-a61b-254774b4437e",
   "metadata": {},
   "source": [
    "## Evaluation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a17cf979-1498-4d1e-b1c8-e8357dfca6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Naive Bayes : \n",
      "\n",
      "Training Score for Naive Bayes :  89.78\n",
      "Testing Score for Naive Bayes : 91.30\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.97      0.95       440\n",
      "           1       0.08      0.03      0.05        31\n",
      "\n",
      "    accuracy                           0.91       471\n",
      "   macro avg       0.51      0.50      0.50       471\n",
      "weighted avg       0.88      0.91      0.89       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[429  11]\n",
      " [ 30   1]]\n",
      "ROC AUC  : 0.5036290322580644\n",
      "\n",
      " KNeighbours : \n",
      "\n",
      "Training Score for KNeighbours :  93.43\n",
      "Testing Score for KNeighbours : 93.42\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      1.00      0.97       440\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.47      0.50      0.48       471\n",
      "weighted avg       0.87      0.93      0.90       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[440   0]\n",
      " [ 31   0]]\n",
      "ROC AUC  : 0.5\n",
      "\n",
      " SVM : \n",
      "\n",
      "Training Score for SVM :  93.34\n",
      "Testing Score for SVM : 93.42\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      1.00      0.97       440\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.47      0.50      0.48       471\n",
      "weighted avg       0.87      0.93      0.90       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[440   0]\n",
      " [ 31   0]]\n",
      "ROC AUC  : 0.5\n",
      "\n",
      " LogisticRegression : \n",
      "\n",
      "Training Score for LogisticRegression :  57.85\n",
      "Testing Score for LogisticRegression : 55.41\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.55      0.70       440\n",
      "           1       0.10      0.68      0.17        31\n",
      "\n",
      "    accuracy                           0.55       471\n",
      "   macro avg       0.53      0.61      0.43       471\n",
      "weighted avg       0.90      0.55      0.66       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[240 200]\n",
      " [ 10  21]]\n",
      "ROC AUC  : 0.6114369501466275\n",
      "\n",
      " DecisionTree : \n",
      "\n",
      "Training Score for DecisionTree :  100.00\n",
      "Testing Score for DecisionTree : 79.19\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.83      0.88       440\n",
      "           1       0.10      0.26      0.14        31\n",
      "\n",
      "    accuracy                           0.79       471\n",
      "   macro avg       0.52      0.54      0.51       471\n",
      "weighted avg       0.89      0.79      0.83       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[365  75]\n",
      " [ 23   8]]\n",
      "ROC AUC  : 0.5438049853372434\n",
      "\n",
      " RandomForest : \n",
      "\n",
      "Training Score for RandomForest :  100.00\n",
      "Testing Score for RandomForest : 93.21\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      1.00      0.96       440\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.47      0.50      0.48       471\n",
      "weighted avg       0.87      0.93      0.90       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[439   1]\n",
      " [ 31   0]]\n",
      "ROC AUC  : 0.49886363636363634\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "classifiers = [['Naive Bayes :', GaussianNB()],\n",
    "               ['KNeighbours :', KNeighborsClassifier()],\n",
    "               ['SVM :', SVC()],\n",
    "               ['LogisticRegression :', LogisticRegression()],\n",
    "               ['DecisionTree :',DecisionTreeClassifier()],\n",
    "               ['RandomForest :',RandomForestClassifier()]]\n",
    "\n",
    "for name,classifier in classifiers:\n",
    "    clf=classifier.fit(X_train,y_train)\n",
    "    y_pred=classifier.predict(X_test)\n",
    "    print(f'\\n {name} \\n')\n",
    "    print(f'Training Score for {name}  {clf.score(X_train,y_train) * 100:.2f}' )\n",
    "    print(f'Testing Score for {name} {clf.score(X_test,y_test) * 100:.2f}' )\n",
    "    print(f'Classification report  \\n {classification_report(y_test,y_pred)}' )\n",
    "    print(f'Confusion matrix  \\n {confusion_matrix(y_test,y_pred)}' )\n",
    "    print(f'ROC AUC  : {roc_auc_score(y_test,y_pred)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e500ec-f7ca-4ce9-a538-69e930fbe290",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
