{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fb44d-af99-4164-bdb5-7a06d2c816d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7e926-f5c7-45f0-b66c-139f97479f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import missingno as msno\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import shapiro\n",
    "# imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url1 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "names = [\"feature\" + str(x) for x in range(1, 591)]\n",
    "df1 = pd.read_csv(url1,sep=\" \", names=names, na_values = \"NaN\",header=None)\n",
    "df1.head()\n",
    "\n",
    "url2 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "df2 = pd.read_csv(url2,sep=\" \",names = [\"Result\",\"Date\"])\n",
    "\n",
    "#df2.columns =['Pass/Fail','Date']\n",
    "df2.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Convertion of Date into Datetime from Object(String) data types\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.dtypes\n",
    "\n",
    "\n",
    "\n",
    "#Joinig TWO df1 and df2 Dataframe naming SECOM\n",
    "Secom = pd.concat([df1,df2],axis = 1)\n",
    "print(Secom)\n",
    "\n",
    "Secom = Secom.drop(['Date'],axis=1)\n",
    "                   \n",
    "# establish target and features of the manufacturing data\n",
    "# set the target to the encoded manufacturing outcome column\n",
    "y = Secom[['Result']]\n",
    "# set the features as the rest of the dataset after dropping the features that are no\n",
    "x = Secom.drop(['Result'], axis=1)\n",
    "\n",
    "# getting the shapes of new data sets x and y\n",
    "print(\"shape of x:\", x.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "#Splitting data\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1,stratify = y)\n",
    "\n",
    "\n",
    "\n",
    "# getting the counts\n",
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train: \", y_train.shape)\n",
    "print(\"shape of y_test: \", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Removing features having Missing ratio more than 50%\n",
    "\n",
    "\n",
    "def percentna(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isnull().sum()/len(dataframe))>threshold]\n",
    "    return columns.tolist()\n",
    "\n",
    " \n",
    "\n",
    "na_columns = percentna(x_train, 0.5)\n",
    "len(na_columns)\n",
    "x_train_dn = x_train.drop(na_columns, axis=1)\n",
    "x_train_dn.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Low Variance Filter\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thres=VarianceThreshold(threshold=0)\n",
    "var_thres.fit(x_train_dn)\n",
    "\n",
    " \n",
    "\n",
    "constant_columns = [column for column in x_train_dn.columns\n",
    "                    if column not in x_train_dn.columns[var_thres.get_support()]]\n",
    "\n",
    "\n",
    "print(len(constant_columns))\n",
    "\n",
    "x_train_lv = x_train_dn.drop(constant_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ed953-73e7-4825-8b49-bfa08d807bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outliers(feat):\n",
    " upper_limit = feat.mean() + 3*feat.std()\n",
    " lower_limit = feat.mean() - 3*feat.std()\n",
    "\n",
    " feat = np.where(\n",
    "    feat >upper_limit,\n",
    "    upper_limit,\n",
    "    np.where(\n",
    "       feat <lower_limit,\n",
    "        lower_limit,\n",
    "        feat ))\n",
    " return feat\n",
    "\n",
    "x_train_outliers_imputation =x_train_lv.copy()\n",
    "for column in x_train_outliers_imputation:\n",
    "  x_train_outliers_imputation[column] = outliers(x_train_outliers_imputation[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e4e73e-e82d-4371-b932-503c74d74485",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hot deck (LOCF - last observation carried forward )\n",
    "numColumns = x_train_outliers_imputation.select_dtypes(include=np.number).columns.tolist();\n",
    "x_train_Hot_deck1 = x_train_outliers_imputation.copy()\n",
    "x_test_Hot_deck1 = x_test.copy()\n",
    "x_train_Hot_deck1[numColumns] = x_train_Hot_deck1[numColumns].fillna(method ='ffill')\n",
    "x_test_Hot_deck1[numColumns] = x_test_Hot_deck1[numColumns].fillna(method ='ffill')\n",
    "\n",
    "#num_cols_with_na = num_cols[x_train_Hot_deck1[num_cols].isnull().mean() > 0]\n",
    "#print(f\"*** numerical columns that have NaN's ({len(num_cols_with_na)}): \\n{num_cols_with_na}\\n\\n\")\n",
    "\n",
    "\n",
    "x_train_Hot_deck1.isnull().mean().sort_values(ascending =False)\n",
    "x_test_Hot_deck1.isnull().mean().sort_values(ascending =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0600a2-ac77-4e16-bfa8-98b40fa50df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BORUTA\n",
    "#REFERENCE: https://github.com/bnsreenu/python_for_microscopists/blob/master/198_Boruta_feature_selection_breast_cancer.py\n",
    "#pip install boruta\n",
    " \n",
    "#Standarize train data for BORUTA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_std = sc.fit_transform(x_train_Hot_deck1)\n",
    "x_test_std = sc.transform(x_test_Hot_deck1)\n",
    " \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    " \n",
    "# load y_train as an array\n",
    " \n",
    "y = y_train.values\n",
    "y = y.ravel()\n",
    " \n",
    "# define random forest classifier, with utilising all cores and\n",
    "# sampling in proportion to y labels\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    " \n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    " \n",
    "# find all relevant features \n",
    "feat_selector.fit(x_train_std, y)\n",
    " \n",
    "# check selected features \n",
    "feat_selector.support_\n",
    " \n",
    "# check ranking of features\n",
    "feat_selector.ranking_\n",
    " \n",
    "# call transform() on X to filter it down to selected features\n",
    "X_filtered = feat_selector.transform(x_train_std)\n",
    " \n",
    "import numpy as np\n",
    "feature_names = np.array(Secom.columns)\n",
    " \n",
    "# Ranked features greater than threshold\n",
    "feature_ranks = list(zip(feature_names, \n",
    " feat_selector.ranking_, \n",
    " feat_selector.support_))\n",
    " \n",
    "# print the results\n",
    "for feat in feature_ranks:\n",
    " print('Feature: {:<30} Rank: {}, Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    " \n",
    "#Now use the subset of features to fit XGBoost model on training data\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier()\n",
    " \n",
    "xgb_model.fit(X_filtered, y_train)\n",
    " \n",
    "#Now predict on test data using the trained model. \n",
    " \n",
    "#First apply feature selector transform to make sure same features are selected from test data\n",
    "X_test_filtered = feat_selector.transform(x_test_std)\n",
    "prediction_xgb = xgb_model.predict(X_test_filtered)\n",
    " \n",
    "#Print overall accuracy\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_xgb))\n",
    " \n",
    "#Confusion Matrix - verify accuracy of each class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, prediction_xgb)\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8ceb6c-7fca-4a1c-a419-a63268cd9d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BORUTA\n",
    "#REFERENCE: https://github.com/bnsreenu/python_for_microscopists/blob/master/198_Boruta_feature_selection_breast_cancer.py\n",
    "#pip install boruta\n",
    " \n",
    "#Standarize train data for BORUTA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_std = sc.fit_transform(x_train)\n",
    "x_test_std = sc.transform(x_test)\n",
    " \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    " \n",
    "# load y_train as an array\n",
    " \n",
    "y = y_train.values\n",
    "y = y.ravel()\n",
    " \n",
    "# define random forest classifier, with utilising all cores and\n",
    "# sampling in proportion to y labels\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    " \n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    " \n",
    "# find all relevant features \n",
    "feat_selector.fit(x_train_std, y)\n",
    " \n",
    "# check selected features \n",
    "feat_selector.support_\n",
    " \n",
    "# check ranking of features\n",
    "feat_selector.ranking_\n",
    " \n",
    "# call transform() on X to filter it down to selected features\n",
    "X_filtered = feat_selector.transform(x_train_std)\n",
    " \n",
    "import numpy as np\n",
    "feature_names = np.array(Secom.columns)\n",
    " \n",
    "# Ranked features greater than threshold\n",
    "feature_ranks = list(zip(feature_names, \n",
    " feat_selector.ranking_, \n",
    " feat_selector.support_))\n",
    " \n",
    "# print the results\n",
    "for feat in feature_ranks:\n",
    " print('Feature: {:<30} Rank: {}, Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    " \n",
    "#Now use the subset of features to fit XGBoost model on training data\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier()\n",
    " \n",
    "xgb_model.fit(X_filtered, y_train)\n",
    " \n",
    "#Now predict on test data using the trained model. \n",
    " \n",
    "#First apply feature selector transform to make sure same features are selected from test data\n",
    "X_test_filtered = feat_selector.transform(x_test_std)\n",
    "prediction_xgb = xgb_model.predict(X_test_filtered)\n",
    " \n",
    "#Print overall accuracy\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_xgb))\n",
    " \n",
    "#Confusion Matrix - verify accuracy of each class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, prediction_xgb)\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
