{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77dea35f-08f1-4243-a078-8651aed44f06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e672e183-dd68-4746-835b-b415eb36c9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature1  feature2   feature3   feature4  feature5  feature6  feature7  \\\n",
      "0      3030.93   2564.00  2187.7333  1411.1265    1.3602     100.0   97.6133   \n",
      "1      3095.78   2465.14  2230.4222  1463.6606    0.8294     100.0  102.3433   \n",
      "2      2932.61   2559.94  2186.4111  1698.0172    1.5102     100.0   95.4878   \n",
      "3      2988.72   2479.90  2199.0333   909.7926    1.3204     100.0  104.2367   \n",
      "4      3032.24   2502.87  2233.3667  1326.5200    1.5334     100.0  100.3967   \n",
      "...        ...       ...        ...        ...       ...       ...       ...   \n",
      "1562   2899.41   2464.36  2179.7333  3085.3781    1.4843     100.0   82.2467   \n",
      "1563   3052.31   2522.55  2198.5667  1124.6595    0.8763     100.0   98.4689   \n",
      "1564   2978.81   2379.78  2206.3000  1110.4967    0.8236     100.0   99.4122   \n",
      "1565   2894.92   2532.01  2177.0333  1183.7287    1.5726     100.0   98.7978   \n",
      "1566   2944.92   2450.76  2195.4444  2914.1792    1.5978     100.0   85.1011   \n",
      "\n",
      "      feature8  feature9  feature10  ...  feature583  feature584  feature585  \\\n",
      "0       0.1242    1.5005     0.0162  ...      0.5005      0.0118      0.0035   \n",
      "1       0.1247    1.4966    -0.0005  ...      0.5019      0.0223      0.0055   \n",
      "2       0.1241    1.4436     0.0041  ...      0.4958      0.0157      0.0039   \n",
      "3       0.1217    1.4882    -0.0124  ...      0.4990      0.0103      0.0025   \n",
      "4       0.1235    1.5031    -0.0031  ...      0.4800      0.4766      0.1045   \n",
      "...        ...       ...        ...  ...         ...         ...         ...   \n",
      "1562    0.1248    1.3424    -0.0045  ...      0.4988      0.0143      0.0039   \n",
      "1563    0.1205    1.4333    -0.0061  ...      0.4975      0.0131      0.0036   \n",
      "1564    0.1208       NaN        NaN  ...      0.4987      0.0153      0.0041   \n",
      "1565    0.1213    1.4622    -0.0072  ...      0.5004      0.0178      0.0038   \n",
      "1566    0.1235       NaN        NaN  ...      0.4987      0.0181      0.0040   \n",
      "\n",
      "      feature586  feature587  feature588  feature589  feature590  Result  \\\n",
      "0         2.3630         NaN         NaN         NaN         NaN      -1   \n",
      "1         4.4447      0.0096      0.0201      0.0060    208.2045      -1   \n",
      "2         3.1745      0.0584      0.0484      0.0148     82.8602       1   \n",
      "3         2.0544      0.0202      0.0149      0.0044     73.8432      -1   \n",
      "4        99.3032      0.0202      0.0149      0.0044     73.8432      -1   \n",
      "...          ...         ...         ...         ...         ...     ...   \n",
      "1562      2.8669      0.0068      0.0138      0.0047    203.1720      -1   \n",
      "1563      2.6238      0.0068      0.0138      0.0047    203.1720      -1   \n",
      "1564      3.0590      0.0197      0.0086      0.0025     43.5231      -1   \n",
      "1565      3.5662      0.0262      0.0245      0.0075     93.4941      -1   \n",
      "1566      3.6275      0.0117      0.0162      0.0045    137.7844      -1   \n",
      "\n",
      "                    Date  \n",
      "0    2008-07-19 11:55:00  \n",
      "1    2008-07-19 12:32:00  \n",
      "2    2008-07-19 13:17:00  \n",
      "3    2008-07-19 14:43:00  \n",
      "4    2008-07-19 15:22:00  \n",
      "...                  ...  \n",
      "1562 2008-10-16 15:13:00  \n",
      "1563 2008-10-16 20:49:00  \n",
      "1564 2008-10-17 05:26:00  \n",
      "1565 2008-10-17 06:01:00  \n",
      "1566 2008-10-17 06:07:00  \n",
      "\n",
      "[1567 rows x 592 columns]\n",
      "shape of x: (1567, 590)\n",
      "shape of y: (1567, 1)\n",
      "shape of x_train:  (1096, 590)\n",
      "shape of x_test:  (471, 590)\n",
      "shape of y_train:  (1096, 1)\n",
      "shape of y_test:  (471, 1)\n",
      "116\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import missingno as msno\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import shapiro\n",
    "# imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url1 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "names = [\"feature\" + str(x) for x in range(1, 591)]\n",
    "df1 = pd.read_csv(url1,sep=\" \", names=names, na_values = \"NaN\",header=None)\n",
    "df1.head()\n",
    "\n",
    "url2 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "df2 = pd.read_csv(url2,sep=\" \",names = [\"Result\",\"Date\"])\n",
    "\n",
    "#df2.columns =['Pass/Fail','Date']\n",
    "df2.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Convertion of Date into Datetime from Object(String) data types\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.dtypes\n",
    "\n",
    "\n",
    "\n",
    "#Joinig TWO df1 and df2 Dataframe naming SECOM\n",
    "Secom = pd.concat([df1,df2],axis = 1)\n",
    "print(Secom)\n",
    "\n",
    "Secom = Secom.drop(['Date'],axis=1)\n",
    "                   \n",
    "# establish target and features of the manufacturing data\n",
    "# set the target to the encoded manufacturing outcome column\n",
    "y = Secom[['Result']]\n",
    "# set the features as the rest of the dataset after dropping the features that are no\n",
    "x = Secom.drop(['Result'], axis=1)\n",
    "\n",
    "# getting the shapes of new data sets x and y\n",
    "print(\"shape of x:\", x.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "#Splitting data\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1,stratify = y)\n",
    "\n",
    "\n",
    "\n",
    "# getting the counts\n",
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train: \", y_train.shape)\n",
    "print(\"shape of y_test: \", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Removing features having Missing ratio more than 50%\n",
    "\n",
    "\n",
    "def percentna(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isnull().sum()/len(dataframe))>threshold]\n",
    "    return columns.tolist()\n",
    "\n",
    " \n",
    "\n",
    "na_columns = percentna(x_train, 0.5)\n",
    "len(na_columns)\n",
    "x_train_dn = x_train.drop(na_columns, axis=1)\n",
    "x_train_dn.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Low Variance Filter\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thres=VarianceThreshold(threshold=0)\n",
    "var_thres.fit(x_train_dn)\n",
    "\n",
    " \n",
    "\n",
    "constant_columns = [column for column in x_train_dn.columns\n",
    "                    if column not in x_train_dn.columns[var_thres.get_support()]]\n",
    "\n",
    "\n",
    "print(len(constant_columns))\n",
    "\n",
    "x_train_lv = x_train_dn.drop(constant_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa9f24e-6145-4ee9-8b6a-9481cd893b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf853a01-2bde-4e32-8620-6a3d90a5242c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR_outliers(data,limit=1.5):\n",
    "    numColumns = data.select_dtypes(include=np.number).columns.tolist(); # extract list of numeric columns\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3-Q1;\n",
    "    outliers=((data[numColumns] < (Q1 - limit*IQR)) | (data[numColumns] > (Q3 + limit*IQR))).sum()*100/data.shape[0]\n",
    "    return outliers \n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "x_train_lv = x_train_lv.copy()\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state= 42)\n",
    "df_outliers = pd.DataFrame(quantile_transformer.fit_transform(x_train_lv),columns=x_train_lv.columns)\n",
    "outliers = IQR_outliers(df_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa5672b-18fe-4de3-98bc-ec6efb2fae95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6df555ef-c146-4894-83b9-d058824e9cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "numColumns = df_outliers.select_dtypes(include=np.number).columns.tolist();\n",
    "\n",
    "# initialize imputer. use strategy='mean' for mean imputation\n",
    "imputer = SimpleImputer(strategy='mean')# fit the imputer on X_train. we pass only numeric columns with NA's here.\n",
    "imputer.fit(df_outliers[numColumns])# transform the data using the fitted imputer\n",
    "X_train_mean_impute = imputer.transform(df_outliers[numColumns])\n",
    "X_test_mean_impute = imputer.transform(x_test[numColumns])# put the output into DataFrame. remember to pass columns used in fit/transform\n",
    "X_train_mean_impute = pd.DataFrame(X_train_mean_impute, columns=numColumns)\n",
    "X_test_mean_impute = pd.DataFrame(X_test_mean_impute, columns=numColumns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ba1486-d9cf-4a8d-858a-ce238542556e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1178c109-048b-4213-aecb-940fbbfd05ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance Matrix \n",
      " [[ 9.55196068e-03 -1.43876050e-03 -1.45726741e-04 ... -1.13432357e-04\n",
      "  -2.38216535e-04 -2.31765256e-04]\n",
      " [-1.43876050e-03  9.56145562e-03  4.47709079e-04 ...  1.39001784e-04\n",
      "  -2.26531994e-05  4.02037004e-04]\n",
      " [-1.45726741e-04  4.47709079e-04  1.03225963e-02 ... -3.23913368e-04\n",
      "  -2.95836748e-04 -3.35893578e-04]\n",
      " ...\n",
      " [-1.13432357e-04  1.39001784e-04 -3.23913368e-04 ...  9.58228897e-03\n",
      "   9.33830592e-03  5.16068331e-03]\n",
      " [-2.38216535e-04 -2.26531994e-05 -2.95836748e-04 ...  9.33830592e-03\n",
      "   9.70891368e-03  5.14963177e-03]\n",
      " [-2.31765256e-04  4.02037004e-04 -3.35893578e-04 ...  5.16068331e-03\n",
      "   5.14963177e-03  1.34640801e-02]]\n",
      "Cumulative Variance Explained [  3.41348      6.31090825   9.01992882  11.06212678  13.02027047\n",
      "  14.82653529  16.55482464  18.2583664   19.91247094  21.5455266\n",
      "  23.12645409  24.63050733  26.11959531  27.50913143  28.84995143\n",
      "  30.16049945  31.43560398  32.65863049  33.8336439   34.95555836\n",
      "  36.06168119  37.14395057  38.18870058  39.20582744  40.19650063\n",
      "  41.16230773  42.12320394  43.04801886  43.95458138  44.83077729\n",
      "  45.69435211  46.54839027  47.38007618  48.18412433  48.98534452\n",
      "  49.7763244   50.55057152  51.3170463   52.06816126  52.81319682\n",
      "  53.53893471  54.25797956  54.97344749  55.66445884  56.35110657\n",
      "  57.03129724  57.70730168  58.37516988  59.0318143   59.67009531\n",
      "  60.29211327  60.90984557  61.52315215  62.11411444  62.69961236\n",
      "  63.2763882   63.84887796  64.42118561  64.97708758  65.52939171\n",
      "  66.07071259  66.60620108  67.13821851  67.66003021  68.17795189\n",
      "  68.68269144  69.1808275   69.67345399  70.15757173  70.6349052\n",
      "  71.10046032  71.56162128  72.01597586  72.46415964  72.90463861\n",
      "  73.34005403  73.77247213  74.20173474  74.621392    75.03247655\n",
      "  75.43242733  75.82816845  76.22143011  76.60281034  76.98253577\n",
      "  77.35771225  77.72811425  78.0927263   78.45349725  78.8096381\n",
      "  79.16129546  79.50500325  79.8476586   80.17884615  80.50355361\n",
      "  80.82657261  81.14537731  81.4606477   81.77071366  82.07445802\n",
      "  82.37337434  82.67062852  82.96344543  83.2528325   83.53736228\n",
      "  83.8190863   84.09304269  84.36376366  84.6297923   84.89227744\n",
      "  85.15094843  85.40577216  85.65418145  85.90129706  86.14463578\n",
      "  86.38328419  86.61592199  86.84718519  87.0761806   87.30140084\n",
      "  87.52497397  87.74823733  87.96696624  88.17862604  88.38644758\n",
      "  88.59169482  88.79338462  88.99213106  89.1893933   89.38274327\n",
      "  89.57442974  89.76379241  89.95038596  90.13462012  90.31685057\n",
      "  90.49811062  90.67423798  90.84751149  91.01725638  91.18526035\n",
      "  91.35057011  91.51337316  91.67264263  91.83101324  91.98843021\n",
      "  92.1446097   92.29747977  92.44927753  92.59769932  92.74386809\n",
      "  92.88897817  93.02905777  93.16733786  93.30473111  93.43891821\n",
      "  93.57174155  93.70278148  93.83330682  93.9597074   94.08552904\n",
      "  94.21000236  94.33270223  94.452916    94.57163504  94.68754487\n",
      "  94.80279815  94.91702898  95.02920437  95.14008772  95.24908137\n",
      "  95.35397334  95.45674753  95.55927225  95.66142301  95.76099539\n",
      "  95.85832672  95.95350266  96.04705601  96.13859056  96.22886224\n",
      "  96.31845034  96.40714591  96.49119965  96.5741857   96.65640678\n",
      "  96.73774848  96.81709017  96.89489058  96.97127985  97.0470262\n",
      "  97.12011886  97.19182484  97.2626882   97.33033093  97.3970135\n",
      "  97.46265006  97.52716209  97.58942834  97.65092413  97.71114807\n",
      "  97.77058862  97.82796305  97.8845606   97.94015731  97.99421926\n",
      "  98.0464532   98.09759071  98.14815008  98.19778078  98.24663515\n",
      "  98.29376971  98.34019256  98.3847345   98.4275649   98.47038128\n",
      "  98.51277955  98.5535484   98.59399426  98.63330007  98.66977838\n",
      "  98.70532211  98.73958188  98.77297837  98.80603111  98.83812359\n",
      "  98.86887229  98.89913419  98.92817539  98.95632973  98.98308512\n",
      "  99.0095824   99.03398446  99.05800675  99.08106386  99.10250237\n",
      "  99.1233742   99.14376801  99.16354254  99.18247061  99.20104468\n",
      "  99.21904942  99.23595164  99.25250593  99.26859319  99.28439301\n",
      "  99.30008627  99.31501017  99.32969919  99.34389689  99.3578568\n",
      "  99.37134226  99.38461047  99.39748457  99.41014633  99.42260987\n",
      "  99.43478117  99.44648853  99.45796124  99.46918331  99.47995675\n",
      "  99.4905806   99.50108786  99.51134125  99.52138131  99.53118907\n",
      "  99.5407724   99.55014705  99.55917341  99.56811199  99.57691776\n",
      "  99.58567963  99.59431286  99.6025891   99.6106899   99.61861022\n",
      "  99.62639662  99.63401206  99.64152753  99.64892877  99.65630543\n",
      "  99.66357074  99.670645    99.67757752  99.68450068  99.69121721\n",
      "  99.69784593  99.70441315  99.71085874  99.7171947   99.72338578\n",
      "  99.72955011  99.73565308  99.74170356  99.74756026  99.75334287\n",
      "  99.75902254  99.76456597  99.76996098  99.77530165  99.78053552\n",
      "  99.78571236  99.79082607  99.79582     99.80060741  99.8053808\n",
      "  99.81010789  99.8147554   99.81927509  99.82370924  99.82810584\n",
      "  99.83242784  99.8366862   99.84077044  99.84479614  99.8487603\n",
      "  99.85258913  99.85632597  99.86000308  99.863633    99.86722055\n",
      "  99.87064819  99.87405176  99.87743121  99.88069915  99.88393884\n",
      "  99.88716673  99.89033731  99.89344696  99.89654518  99.89955546\n",
      "  99.90248302  99.90531267  99.90809162  99.9108465   99.91352092\n",
      "  99.91615916  99.9187731   99.92133904  99.92382046  99.9262827\n",
      "  99.9286824   99.93105     99.93338316  99.93566047  99.93787732\n",
      "  99.94007229  99.94217997  99.94427253  99.94630162  99.94827897\n",
      "  99.95017183  99.95202283  99.9538225   99.95555526  99.95727515\n",
      "  99.95893476  99.96055646  99.96210007  99.96361632  99.96510191\n",
      "  99.96653295  99.96792284  99.96926868  99.97058848  99.97188669\n",
      "  99.97312222  99.97433612  99.97552426  99.97668889  99.97782213\n",
      "  99.97893616  99.98001657  99.98102184  99.98201216  99.98295452\n",
      "  99.98385777  99.9847455   99.98555072  99.98634321  99.98711072\n",
      "  99.98785049  99.98854812  99.98922352  99.98986795  99.9904598\n",
      "  99.99103935  99.99157588  99.99209021  99.99259735  99.99309357\n",
      "  99.99358352  99.99404859  99.99448016  99.99490195  99.99530521\n",
      "  99.99568795  99.99604677  99.99637862  99.99669924  99.99698817\n",
      "  99.99723911  99.99747824  99.99770741  99.9978803   99.99803724\n",
      "  99.99817649  99.99830142  99.99842319  99.99854144  99.9986552\n",
      "  99.99876026  99.9988616   99.99896175  99.99905564  99.99914397\n",
      "  99.99922263  99.99929299  99.99936129  99.99942634  99.99948902\n",
      "  99.99954283  99.99959325  99.99964096  99.99968415  99.99972247\n",
      "  99.99975302  99.99978224  99.99980809  99.99983245  99.999855\n",
      "  99.99987587  99.99989519  99.99991413  99.99993091  99.99994458\n",
      "  99.99995695  99.99996835  99.99997898  99.99998638  99.99999319\n",
      "  99.99999939  99.99999974  99.99999994  99.99999997 100.\n",
      " 100.         100.         100.         100.         100.        ]\n",
      "           0         1         2         3         4         5         6    \\\n",
      "0     0.492882 -0.391207  0.420807 -0.171236  0.053805 -0.421629 -0.201882   \n",
      "1     0.390447  0.271377 -0.718675 -0.009007  0.014068  0.118501 -0.145888   \n",
      "2    -0.514243  0.336498  0.226552 -0.503188  0.150291 -0.525936 -0.244018   \n",
      "3    -0.451464 -0.357992  0.270151  0.402542 -0.509751  0.107021  0.110871   \n",
      "4    -0.652069  0.399481  0.228212 -0.348278 -0.051438 -0.301478  0.310920   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "1091 -0.134508  0.488370  0.234501  0.264233 -0.353215 -0.147155  0.112882   \n",
      "1092  0.517560 -0.085599 -0.445548 -0.242627  0.178498  0.287207 -0.197635   \n",
      "1093 -0.361143 -0.179524 -0.443113 -0.584190 -0.182343  0.442233  0.013034   \n",
      "1094  0.412901  0.189939  0.117583 -0.297220 -0.061061  0.083126  0.183793   \n",
      "1095 -0.089891 -0.855075  0.232621  0.085142 -0.300236  0.331878  0.366751   \n",
      "\n",
      "           7         8         9    ...       123       124       125  \\\n",
      "0    -0.146427 -0.243951 -0.266471  ... -0.117512 -0.028008 -0.141325   \n",
      "1    -0.262645 -0.393251  0.348377  ...  0.038600  0.045078  0.042562   \n",
      "2    -0.178925 -0.336121 -0.418271  ... -0.230479  0.091106  0.064357   \n",
      "3     0.010358 -0.169102  0.041984  ... -0.128913 -0.092480 -0.013628   \n",
      "4     0.044341 -0.110811 -0.091959  ... -0.002491  0.073050 -0.071559   \n",
      "...        ...       ...       ...  ...       ...       ...       ...   \n",
      "1091 -0.151702  0.439045 -0.146701  ...  0.069288 -0.076612  0.057729   \n",
      "1092  0.208607  0.280933 -0.232022  ...  0.052121  0.008124  0.003164   \n",
      "1093 -0.326673  0.187907  0.166267  ... -0.219608 -0.136516  0.039374   \n",
      "1094  0.277152  0.091912 -0.048293  ...  0.032181  0.037459 -0.061119   \n",
      "1095 -0.005726  0.246212 -0.148969  ...  0.067995 -0.039275  0.002108   \n",
      "\n",
      "           126       127       128       129       130       131       132  \n",
      "0     0.109891 -0.115944 -0.074814 -0.063524 -0.089146 -0.038777  0.068528  \n",
      "1    -0.035041 -0.094864 -0.086613 -0.039304 -0.106422 -0.015041 -0.131148  \n",
      "2    -0.057371 -0.020360  0.025353 -0.071765 -0.018057 -0.075808  0.100907  \n",
      "3     0.000298  0.068158  0.059283 -0.054016 -0.008439  0.020841 -0.052121  \n",
      "4    -0.004784  0.050245 -0.005051 -0.014875  0.103119  0.091574 -0.058803  \n",
      "...        ...       ...       ...       ...       ...       ...       ...  \n",
      "1091 -0.087332  0.032190  0.037653  0.069467  0.010090 -0.166809 -0.015510  \n",
      "1092 -0.067459  0.073538 -0.063777  0.066070  0.121376  0.004344 -0.079033  \n",
      "1093  0.053983  0.108353  0.203718 -0.170934 -0.065508 -0.196763 -0.219924  \n",
      "1094  0.066016  0.021746 -0.200821 -0.080153 -0.120606  0.115198 -0.077737  \n",
      "1095  0.077610  0.058634 -0.116852  0.219118 -0.031181  0.002088 -0.081195  \n",
      "\n",
      "[1096 rows x 133 columns]\n"
     ]
    }
   ],
   "source": [
    "#x_train_Hot_deck2 = x_train_Hot_deck2.replace(np.NaN, 0)\n",
    "#x_train_Hot_deck2.isnull().any().any()\n",
    "#x_test_Hot_deck2 = x_test_Hot_deck2.replace(np.NaN, 0)\n",
    "#x_test_Hot_deck2.isnull().any().any()\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "col=list(X_train_mean_impute.columns)\n",
    "col1=list(X_test_mean_impute.columns)\n",
    "scaler = MinMaxScaler()\n",
    "scaled_feature = scaler.fit_transform(X_train_mean_impute)\n",
    "scaled_feature = pd.DataFrame(scaled_feature,columns=col)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "# Creating a covariance matrix\n",
    "\n",
    "cov_matrix = np.cov(scaled_feature.T)\n",
    "print('Covariance Matrix \\n', cov_matrix)\n",
    "      \n",
    "#perform an eigendecomposition on the covariance matrix:\n",
    "\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov_matrix)  \n",
    "\n",
    "\n",
    "tot = sum(eig_vals)\n",
    "var_exp = [( i /tot ) * 100 for i in sorted(eig_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(\"Cumulative Variance Explained\", cum_var_exp)\n",
    "\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12 ,6))\n",
    "plt.bar(range(1, eig_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\n",
    "plt.step(range(1, eig_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.xlabel('Principal Components')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "pca = PCA(n_components=scaled_feature.shape[1],random_state=1).fit(scaled_feature.values)\n",
    "a = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "i = 0\n",
    "while a[i] < 0.9:\n",
    "    i = i+1\n",
    "f'Number of dimensions needed to capture 90%% of variance:{i}'\n",
    "\n",
    "\n",
    "# PCA with reduced number of components\n",
    "\n",
    "pca = PCA(n_components=i,random_state=1)\n",
    "pca.fit(scaled_feature)\n",
    "#print(pca.components_)\n",
    "\n",
    "pca_df= pd.DataFrame(pca.fit_transform(scaled_feature))\n",
    "print(pca_df)\n",
    "\n",
    "\n",
    "pca_df.shape\n",
    "\n",
    "#heatmap before and after PCA\n",
    "# after PCA shows that the components are in the acceptable range of colinearity: means that the components are independent and  suitable for model making.\n",
    "f, (ax_heatmap, ax_heatmap2) = plt.subplots(2,figsize=(16,12))\n",
    "ax_heatmap.set_title('Before PCA')\n",
    "ax_heatmap2.set_title('After PCA')\n",
    "\n",
    "sns.heatmap(scaled_feature,ax=ax_heatmap,cmap='plasma');\n",
    "sns.heatmap(pca_df,ax=ax_heatmap2,cmap='plasma');\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "# create a PCA object\n",
    "pca = PCA(n_components = i)# extracted features we want to end up within our new dataset(2).\n",
    "# Apply the above object to our training dataset using the fit method.\n",
    "X_train = pca.fit_transform(X_train_mean_impute)\n",
    "# Apply the PCA object to the test set only to transform this set\n",
    "X_test = pca.fit_transform(X_test_mean_impute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bd0d01-09be-45c1-9bdd-b47633722f09",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Evaluation Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "91c07239-a148-4905-9f8c-60792aceaa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Naive Bayes : \n",
      "\n",
      "Training Score for Naive Bayes :  89.96\n",
      "Testing Score for Naive Bayes : 91.51\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.94      0.97      0.96       440\n",
      "           1       0.15      0.06      0.09        31\n",
      "\n",
      "    accuracy                           0.92       471\n",
      "   macro avg       0.55      0.52      0.52       471\n",
      "weighted avg       0.89      0.92      0.90       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[429  11]\n",
      " [ 29   2]]\n",
      "ROC AUC  : 0.519758064516129\n",
      "\n",
      " KNeighbours : \n",
      "\n",
      "Training Score for KNeighbours :  93.43\n",
      "Testing Score for KNeighbours : 93.42\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      1.00      0.97       440\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.47      0.50      0.48       471\n",
      "weighted avg       0.87      0.93      0.90       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[440   0]\n",
      " [ 31   0]]\n",
      "ROC AUC  : 0.5\n",
      "\n",
      " SVM : \n",
      "\n",
      "Training Score for SVM :  93.34\n",
      "Testing Score for SVM : 93.42\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      1.00      0.97       440\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.47      0.50      0.48       471\n",
      "weighted avg       0.87      0.93      0.90       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[440   0]\n",
      " [ 31   0]]\n",
      "ROC AUC  : 0.5\n",
      "\n",
      " LogisticRegression : \n",
      "\n",
      "Training Score for LogisticRegression :  58.67\n",
      "Testing Score for LogisticRegression : 55.41\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.95      0.55      0.70       440\n",
      "           1       0.09      0.61      0.15        31\n",
      "\n",
      "    accuracy                           0.55       471\n",
      "   macro avg       0.52      0.58      0.43       471\n",
      "weighted avg       0.90      0.55      0.66       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[242 198]\n",
      " [ 12  19]]\n",
      "ROC AUC  : 0.5814516129032258\n",
      "\n",
      " DecisionTree : \n",
      "\n",
      "Training Score for DecisionTree :  100.00\n",
      "Testing Score for DecisionTree : 79.62\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      0.84      0.89       440\n",
      "           1       0.05      0.13      0.08        31\n",
      "\n",
      "    accuracy                           0.80       471\n",
      "   macro avg       0.49      0.49      0.48       471\n",
      "weighted avg       0.87      0.80      0.83       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[371  69]\n",
      " [ 27   4]]\n",
      "ROC AUC  : 0.4861070381231671\n",
      "\n",
      " RandomForest : \n",
      "\n",
      "Training Score for RandomForest :  100.00\n",
      "Testing Score for RandomForest : 93.42\n",
      "Classification report  \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.93      1.00      0.97       440\n",
      "           1       0.00      0.00      0.00        31\n",
      "\n",
      "    accuracy                           0.93       471\n",
      "   macro avg       0.47      0.50      0.48       471\n",
      "weighted avg       0.87      0.93      0.90       471\n",
      "\n",
      "Confusion matrix  \n",
      " [[440   0]\n",
      " [ 31   0]]\n",
      "ROC AUC  : 0.5\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "#from sklearn.metrics import scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "classifiers = [['Naive Bayes :', GaussianNB()],\n",
    "               ['KNeighbours :', KNeighborsClassifier()],\n",
    "               ['SVM :', SVC()],\n",
    "               ['LogisticRegression :', LogisticRegression()],\n",
    "               ['DecisionTree :',DecisionTreeClassifier()],\n",
    "               ['RandomForest :',RandomForestClassifier()]]\n",
    "\n",
    "for name,classifier in classifiers:\n",
    "    clf=classifier.fit(X_train,y_train)\n",
    "    y_pred=classifier.predict(X_test)\n",
    "    print(f'\\n {name} \\n')\n",
    "    print(f'Training Score for {name}  {clf.score(X_train,y_train) * 100:.2f}' )\n",
    "    print(f'Testing Score for {name} {clf.score(X_test,y_test) * 100:.2f}' )\n",
    "    print(f'Classification report  \\n {classification_report(y_test,y_pred)}' )\n",
    "    print(f'Confusion matrix  \\n {confusion_matrix(y_test,y_pred)}' )\n",
    "    print(f'ROC AUC  : {roc_auc_score(y_test,y_pred)}' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb60264-9031-49f4-b567-6641537f5add",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
