{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ec556d-2cac-46e8-9ea4-77e6e34bf684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b5622-8935-49e9-b7aa-c2926e25c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "from pandas_profiling import ProfileReport\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import missingno as msno\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import shapiro\n",
    "# imputer\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "url1 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom.data'\n",
    "names = [\"feature\" + str(x) for x in range(1, 591)]\n",
    "df1 = pd.read_csv(url1,sep=\" \", names=names, na_values = \"NaN\",header=None)\n",
    "df1.head()\n",
    "\n",
    "url2 ='https://archive.ics.uci.edu/ml/machine-learning-databases/secom/secom_labels.data'\n",
    "df2 = pd.read_csv(url2,sep=\" \",names = [\"Result\",\"Date\"])\n",
    "\n",
    "#df2.columns =['Pass/Fail','Date']\n",
    "df2.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Convertion of Date into Datetime from Object(String) data types\n",
    "df2['Date'] = pd.to_datetime(df2['Date'])\n",
    "df2.dtypes\n",
    "\n",
    "\n",
    "\n",
    "#Joinig TWO df1 and df2 Dataframe naming SECOM\n",
    "Secom = pd.concat([df1,df2],axis = 1)\n",
    "print(Secom)\n",
    "\n",
    "Secom = Secom.drop(['Date'],axis=1)\n",
    "                   \n",
    "# establish target and features of the manufacturing data\n",
    "# set the target to the encoded manufacturing outcome column\n",
    "y = Secom[['Result']]\n",
    "# set the features as the rest of the dataset after dropping the features that are no\n",
    "x = Secom.drop(['Result'], axis=1)\n",
    "\n",
    "# getting the shapes of new data sets x and y\n",
    "print(\"shape of x:\", x.shape)\n",
    "print(\"shape of y:\", y.shape)\n",
    "\n",
    "#Splitting data\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 1,stratify = y)\n",
    "\n",
    "\n",
    "\n",
    "# getting the counts\n",
    "print(\"shape of x_train: \", x_train.shape)\n",
    "print(\"shape of x_test: \", x_test.shape)\n",
    "print(\"shape of y_train: \", y_train.shape)\n",
    "print(\"shape of y_test: \", y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Removing features having Missing ratio more than 50%\n",
    "\n",
    "\n",
    "def percentna(dataframe, threshold):\n",
    "    columns = dataframe.columns[(dataframe.isnull().sum()/len(dataframe))>threshold]\n",
    "    return columns.tolist()\n",
    "\n",
    " \n",
    "\n",
    "na_columns = percentna(x_train, 0.5)\n",
    "len(na_columns)\n",
    "x_train_dn = x_train.drop(na_columns, axis=1)\n",
    "x_train_dn.shape\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Low Variance Filter\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "var_thres=VarianceThreshold(threshold=0)\n",
    "var_thres.fit(x_train_dn)\n",
    "\n",
    " \n",
    "\n",
    "constant_columns = [column for column in x_train_dn.columns\n",
    "                    if column not in x_train_dn.columns[var_thres.get_support()]]\n",
    "\n",
    "\n",
    "print(len(constant_columns))\n",
    "\n",
    "x_train_lv = x_train_dn.drop(constant_columns,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236583c-b194-434c-83c6-f77e86829031",
   "metadata": {},
   "outputs": [],
   "source": [
    "def IQR_outliers(data,limit=1.5):\n",
    "    numColumns = data.select_dtypes(include=np.number).columns.tolist(); # extract list of numeric columns\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3-Q1;\n",
    "    outliers=((data[numColumns] < (Q1 - limit*IQR)) | (data[numColumns] > (Q3 + limit*IQR))).sum()*100/data.shape[0]\n",
    "    return outliers \n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "x_train_lv = x_train_lv.copy()\n",
    "quantile_transformer = QuantileTransformer(output_distribution='normal', random_state= 42)\n",
    "df_outliers = pd.DataFrame(quantile_transformer.fit_transform(x_train_lv),columns=x_train_lv.columns)\n",
    "outliers = IQR_outliers(df_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a12cbd-c0dd-4b64-9c6c-aa1bd5b81d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize imputer\n",
    "imputer = KNNImputer()\n",
    "\n",
    "\n",
    "\n",
    "# fit the imputer on X_train. pass only numeric columns.\n",
    "imputer.fit(df_outliers[numColumns])\n",
    "\n",
    "\n",
    "\n",
    "# transform the data using the fitted imputer\n",
    "X_train_knn_impute2 = imputer.transform(df_outliers[numColumns])\n",
    "X_test_knn_impute2 = imputer.transform(x_test[numColumns])\n",
    "\n",
    "\n",
    "\n",
    "# put the output into DataFrame. remember to pass columns used in fit/transform\n",
    "X_train_knn_impute2 = pd.DataFrame(X_train_knn_impute2, columns=numColumns)\n",
    "X_test_knn_impute2 = imputer.transform(x_test[numColumns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e03948-8ddb-40c9-a912-36263daf8049",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e4762-7954-4a87-ac1b-46a1bf59ca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BORUTA\n",
    "#REFERENCE: https://github.com/bnsreenu/python_for_microscopists/blob/master/198_Boruta_feature_selection_breast_cancer.py\n",
    "#pip install boruta\n",
    " \n",
    "#Standarize train data for BORUTA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "x_train_std = sc.fit_transform(X_train_knn_impute2)\n",
    "x_test_std = sc.transform(X_test_knn_impute2)\n",
    " \n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from boruta import BorutaPy\n",
    " \n",
    "# load y_train as an array\n",
    " \n",
    "y = y_train.values\n",
    "y = y.ravel()\n",
    " \n",
    "# define random forest classifier, with utilising all cores and\n",
    "# sampling in proportion to y labels\n",
    "rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)\n",
    " \n",
    "# define Boruta feature selection method\n",
    "feat_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=1)\n",
    " \n",
    "# find all relevant features \n",
    "feat_selector.fit(x_train_std, y)\n",
    " \n",
    "# check selected features \n",
    "feat_selector.support_\n",
    " \n",
    "# check ranking of features\n",
    "feat_selector.ranking_\n",
    " \n",
    "# call transform() on X to filter it down to selected features\n",
    "X_filtered = feat_selector.transform(x_train_std)\n",
    " \n",
    "import numpy as np\n",
    "feature_names = np.array(Secom.columns)\n",
    " \n",
    "# Ranked features greater than threshold\n",
    "feature_ranks = list(zip(feature_names, \n",
    " feat_selector.ranking_, \n",
    " feat_selector.support_))\n",
    " \n",
    "# print the results\n",
    "for feat in feature_ranks:\n",
    " print('Feature: {:<30} Rank: {}, Keep: {}'.format(feat[0], feat[1], feat[2]))\n",
    " \n",
    "#Now use the subset of features to fit XGBoost model on training data\n",
    "import xgboost as xgb\n",
    "xgb_model = xgb.XGBClassifier()\n",
    " \n",
    "xgb_model.fit(X_filtered, y_train)\n",
    " \n",
    "#Now predict on test data using the trained model. \n",
    " \n",
    "#First apply feature selector transform to make sure same features are selected from test data\n",
    "X_test_filtered = feat_selector.transform(x_test_std)\n",
    "prediction_xgb = xgb_model.predict(X_test_filtered)\n",
    " \n",
    "#Print overall accuracy\n",
    "from sklearn import metrics\n",
    "print (\"Accuracy = \", metrics.accuracy_score(y_test, prediction_xgb))\n",
    " \n",
    "#Confusion Matrix - verify accuracy of each class\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, prediction_xgb)\n",
    "#print(cm)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
